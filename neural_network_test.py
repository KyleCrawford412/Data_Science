# -*- coding: utf-8 -*-
"""Neural_Network_Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G13OrUrv6d0lpv7oextwduN1jkJNAnmj
"""

!pip install fredapi

import pandas as pd
import matplotlib.pyplot as plt
from fredapi import Fred
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models

# set up the FRED API connection with your API key
fred = Fred(api_key='e62a5d114bc51033917d60f16d0799dd')

# retrieve GDP data with valid series ID and optional start and end dates
gdp = fred.get_series('GDPC1', start_date='1990-01-01', end_date='2023-03-01')

# convert data to a pandas DataFrame
data = pd.DataFrame({'GDP': gdp})

# plot the data
fig, ax = plt.subplots()
ax.plot(data.index, data['GDP'])
ax.set_title('U.S. GDP over time')
ax.set_xlabel('Year')
ax.set_ylabel('GDP (in trillions of dollars)')
ax.set_xlim(pd.Timestamp('1990-01-01'), pd.Timestamp('2023-01-01'))
plt.show()

from sklearn.linear_model import LinearRegression

# retrieve the data for the variables of interest
gdp = fred.get_series('GDPC1') # GDP
hpi = fred.get_series('SP500') # S&P 500 index
mortgage_rate = fred.get_series('MORTGAGE30US') # 30-year fixed mortgage rate
median_income = fred.get_series('MEHOINUSA672N') # Median household income
mspus = fred.get_series('MSPUS', start_date='1990-01-01', end_date='2023-03-01')

# convert data to a pandas DataFrame
data = pd.DataFrame({'GDP': gdp, "HPI" : hpi, "MORTGAGE_RATE" : mortgage_rate, "MEDIAN_INCOME" : median_income, 'MSPUS': mspus})

# create a new column for the house price index (HPI) lagged by one quarter
data['HPI_LAGGED'] = data['HPI'].shift(1)

# drop the first row, which contains a NaN value due to the shift
data = data.dropna()

data.head()

import seaborn as sns

# Connect to the FRED API using your API key
fred = Fred(api_key='e62a5d114bc51033917d60f16d0799dd')

# Define the variable codes for the median house price, personal income, and mortgage rate data
house_price_var = 'MEDLISPRIALLSFRB'
income_var = 'MEPAINUSA672N'
population_var = 'POPTHM'
unemployment_var = 'UNRATE'
construction_var = 'HOUST'
inflation_var = 'CPALTT01USM657N'
homeownership_var = 'RHORUSQ156N'
consumer_debt_var = 'TTLCONS'
consumer_confidence_var = 'UMCSENT'

# Set the start and end dates for the data
start_date = '1963-01-01'
end_date = '2023-01-01'

# Retrieve the median house price, personal income, mortgage rate, and population data
mspus = fred.get_series('MSPUS', start_date, end_date)
income_data = fred.get_series(income_var, start_date, end_date)
population_data = fred.get_series(population_var, start_date, end_date)
gdp = fred.get_series('GDPC1', start_date, end_date) # GDP
unemployment_rate_data = fred.get_series(unemployment_var, start_date, end_date)
inflation_data = fred.get_series(inflation_var, start_date, end_date)
homeownership_data = fred.get_series(homeownership_var, start_date, end_date)
consumer_debt_data = fred.get_series(consumer_debt_var, start_date, end_date)
consumer_confidence_data = fred.get_series(consumer_confidence_var, start_date, end_date)

# Combine the data into a single DataFrame
housing = pd.DataFrame({'house_price': mspus, 'population': population_data, 'gdp': gdp,'unemployement rate': unemployment_rate_data, 'income': income_data, 'inflation': inflation_data, 'homeownership': homeownership_data, 'consumer_debt': consumer_debt_data, 'consumer_confidence': consumer_confidence_data})

# Compute the correlations between the variables
corr = housing.corr()

# Plot a heatmap of the correlations
sns.set(font_scale=1.2)
sns.heatmap(corr, annot=True, cmap='YlGnBu', linewidths=0.5, fmt='.2f',  annot_kws={'size': 8})
plt.title('Correlation Heatmap')
plt.show()

# split data into X and y
X = housing.drop(['house_price'], axis=1)
y = housing['house_price']

# drop rows with missing values
temp = housing.dropna()
X = X.dropna()
y = y.dropna()

# turn data into data array
arr = np.array(temp)

# find the size of the array
print(len(arr))

from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense



# Split the data into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(arr[:, :-1], arr[:, -1], test_size=0.2)

# Define the number of timesteps and features for the input data
num_timesteps = train_data.shape[1]
num_features = 1

# Define the CNN architecture
model = Sequential()
model.add(Conv1D(32, 3, activation='relu', input_shape=(num_timesteps, num_features)))
model.add(MaxPooling1D(2))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(train_data, train_labels, epochs=50, batch_size=4)

# Evaluate the model on the test set
test_loss = model.evaluate(test_data, test_labels)
print('Test loss:', test_loss)

# Make predictions on new data
#new_data = np.array([...])
#predictions = model.predict(new_data)

import numpy as np
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense


# Split the data into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(arr[:, :-1], arr[:, -1], test_size=0.2)

# Define the number of timesteps and features for the input data
num_timesteps = train_data.shape[1]
num_features = train_data.shape[1]

# Define the CNN architecture
model = Sequential()
model.add(Conv1D(32, 3, activation='relu', input_shape=(num_timesteps, num_features)))
model.add(MaxPooling1D(2))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(train_data, train_labels, epochs=50, batch_size=4)

# Evaluate the model on the test set
test_loss = model.evaluate(test_data, test_labels)
print('Test loss:', test_loss)

# Make predictions on new data
#new_data = np.array([...])
#predictions = model.predict(new_data)

# try to impute the data instead of dropping it
from sklearn.impute import SimpleImputer

# Check which columns have missing values
cols_with_missing_values = housing.columns[housing.isna().any()].tolist()

# Create the imputer object
imputer = SimpleImputer(strategy='mean')

print(cols_with_missing_values)

# Impute the missing values
housing[cols_with_missing_values] = imputer.fit_transform(housing[cols_with_missing_values])

# turn data into data array
arr = np.array(housing)

# check empty values
count_empty = housing.isnull().sum()
print(count_empty)

# try model with imputed values
temp = housing

# turn data into data array
arr = np.array(temp)

# Split the data into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(arr[:, :-1], arr[:, -1], test_size=0.2)

# Define the number of timesteps and features for the input data
num_timesteps = train_data.shape[1]
num_features = 1

# Define the CNN architecture
model = Sequential()
model.add(Conv1D(32, 3, activation='relu', input_shape=(num_timesteps, num_features)))
model.add(MaxPooling1D(2))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mse')


# Train the model
model.fit(train_data, train_labels, epochs=50, batch_size=4)

# Evaluate the model on the test set
test_loss = model.evaluate(test_data, test_labels)
print('Test loss:', test_loss)

# Make predictions on new data

# estimations of future economic data
new_data = np.array([[355100, 24302010, 4.3, 75000, .02, 64.8, 6400000, 870],
                     [373500, 28063300, 4.7, 85000, .019, 63.7, 7600000, 75],
                     [388000, 32119290, 4.7, 95000, .020, 62.5, 9000000, 87]])
prediction = model.predict(new_data)
print(prediction)

from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor()

forest.fit(train_data, train_labels)

forest.score(test_data, test_labels)

# Decision Tree Model
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import accuracy_score


# split data into X and y
X = housing.drop(['house_price'], axis=1)
y = housing['house_price']

# training and test sets
# test set 30% again
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)


# initialize decision tree
tree = DecisionTreeRegressor()

# fit data
tree.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score

# make predictions on the test set
y_pred = model.predict(X_test)

# calculate mean squared error
mse = mean_squared_error(y_test, y_pred)

# calculate R² score
r2 = r2_score(y_test, y_pred)

print('Mean squared error:', mse)
print('R² score:', r2)

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Load your data into X and y
# Split the data into training and testing sets

# Create a decision tree regressor
regressor = DecisionTreeRegressor()

# Define the hyperparameters you want to search over
hyperparameters = {'max_depth': [3, 5, 7, 9],
                   'min_samples_split': [2, 4, 6, 8],
                   'min_samples_leaf': [1, 2, 4, 6]}

# Perform a grid search to find the best hyperparameters
grid_search = GridSearchCV(regressor, hyperparameters, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print(grid_search.best_params_)

# Use the best hyperparameters to create a new decision tree regressor
regressor = DecisionTreeRegressor(max_depth=grid_search.best_params_['max_depth'],
                                   min_samples_split=grid_search.best_params_['min_samples_split'],
                                   min_samples_leaf=grid_search.best_params_['min_samples_leaf'])
regressor.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = regressor.predict(X_test)

# Evaluate the model's performance using the mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean squared error:", mse)

predicted_value = regressor.predict(new_data)
print(predicted_value)

# get feature importances
importances = regressor.feature_importances_

# sort features by importance score
indices = np.argsort(importances)[::-1]
sorted_features = [X_train.columns[i] for i in indices]

# plot feature importances
plt.bar(range(X_train.shape[1]), importances[indices])
plt.xticks(range(X_train.shape[1]), sorted_features, rotation=90)
plt.title('Feature Importances')
plt.show()